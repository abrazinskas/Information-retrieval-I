{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Support functions and classes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy.stats as stat\n",
    "from timeit import default_timer as timer\n",
    "import scipy.misc\n",
    "import scipy\n",
    "import operator\n",
    "import gensim\n",
    "import logging\n",
    "import sys\n",
    "from scipy import spatial\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import collections\n",
    "from enum import Enum\n",
    "import itertools\n",
    "import glob\n",
    "import cython\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['legend.loc'] = 'best'\n",
    "import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### support classes and functions\n",
    "\n",
    "# for floats comparison\n",
    "def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n",
    "    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n",
    "\n",
    "\n",
    "def getDocuments(filename, processed=False,max_docs=None):\n",
    "    print('reading documents')\n",
    "    if (processed == False):\n",
    "        vocab = Vocabulary()\n",
    "        pr = Preprocessor(vocab)\n",
    "    infile = open(filename)\n",
    "    raw = infile.read()\n",
    "    infile.close()\n",
    "    res = {}\n",
    "    soup = BeautifulSoup(raw, \"lxml\")\n",
    "    for i,doc in enumerate(soup.findAll(\"doc\")):\n",
    "        if(max_docs!=None and max_docs==i): break\n",
    "\n",
    "        headline = doc.find('headline')\n",
    "        maintext = doc.find('text')\n",
    "        headline = headline = headline.text if headline else \"\"\n",
    "        maintext = maintext = maintext.text if maintext else \"\"\n",
    "\n",
    "        if (processed == False):\n",
    "            headline = pr.prep(headline)\n",
    "            maintext = pr.prep(maintext)\n",
    "        else:\n",
    "            headline=headline.split(\" \")\n",
    "            maintext=maintext.split(\" \")\n",
    "\n",
    "        # for empty sections\n",
    "        if(len(headline)==1 and headline[0]==\"\"):\n",
    "            headline=[]\n",
    "\n",
    "        if(len(maintext)==1 and maintext[0]==\"\"):\n",
    "            maintext=[]\n",
    "\n",
    "        docno = doc.find('docno').text\n",
    "\n",
    "        # making sure that we have ints\n",
    "        headline = [int(x) for x in headline]\n",
    "        maintext = [int(x) for x in maintext]\n",
    "\n",
    "        res[docno.strip()] = {'headline': headline, 'maintext': maintext, 'fulltext': headline +maintext}\n",
    "\n",
    "    print('finished reading documents')\n",
    "    if (processed == False):\n",
    "        vocab.add(\"UNKNOWN\")\n",
    "        return (res, vocab)\n",
    "    else:\n",
    "        return (res)\n",
    "\n",
    "\n",
    "def getQueries(filename, vocab):\n",
    "    queries = {}\n",
    "    pr = Preprocessor(vocab)\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            parts = line.split(\" \")\n",
    "            # parts.pop(0) # remove id\n",
    "            queries[parts[0]] = pr.prep(' '.join(parts), 1)\n",
    "    return queries\n",
    "\n",
    "\n",
    "def getDocQuerRel(filename):\n",
    "    qdr = {}  # maps: qid->{did->rel}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            parts = line.split(\" \")\n",
    "            if (parts[0] not in qdr):\n",
    "                qdr[parts[0]] = {}\n",
    "            qdr[parts[0]][parts[2]] = int(parts[3])\n",
    "    return qdr\n",
    "\n",
    "\n",
    "# writes preprocessed documents into a file\n",
    "def writeDocuments(docs, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for did in docs:\n",
    "            headline = ' '.join(str(x) for x in docs[did]['headline'])\n",
    "            maintext = ' '.join(str(x) for x in docs[did]['maintext'])\n",
    "            doc = \"<doc>\" \\\n",
    "                  \"<docno>\" + did + \"</docno>\" \\\n",
    "                                    \"<headline>\" + headline + \"</headline>\" \\\n",
    "                                                              \"<text>\" + maintext + \"</text>\" \\\n",
    "                                                                                    \"</doc>\"\n",
    "            f.write(doc + \"\\n\")\n",
    "    print(\"Documents written to \" + output_file)\n",
    "\n",
    "\n",
    "def writeVocabulary(vocab, output_file, sep=' '):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"\\n\".join([sep.join((word, str(vocab.wti[word]))) for word in vocab.wti]))\n",
    "    print(\"Vocabulary written to \" + output_file)\n",
    "\n",
    "\n",
    "# writes in the following format:\n",
    "# ndcg for each query\n",
    "def writeNDCG(nDCGs,file):\n",
    "    with open(file, 'w') as f:\n",
    "            f.write(\"\\n\".join([str(nDCG) for nDCG in nDCGs]))\n",
    "    print(\"NDCGS written to \" +file)\n",
    "\n",
    "def readVocabulary(filename, sep=' '):\n",
    "    vocab = Vocabulary()\n",
    "    l = fileLen(filename)\n",
    "    vocab.curr_ind=l\n",
    "    vocab.itw = [\"\" for x in range(l)]\n",
    "    with open(filename) as f:\n",
    "        for word in itertools.islice(f, 0, l):\n",
    "            splt = word.split(sep)\n",
    "            vocab.wti[splt[0]] = int(splt[1])\n",
    "            vocab.itw[int(splt[1])] = splt[0]\n",
    "    return vocab\n",
    "\n",
    "# returns a list of relevances\n",
    "def map_to_relevance(qid, dids, qdr):\n",
    "    relCol = []\n",
    "    for did in dids:\n",
    "        rel = qdr[qid][did] if did in qdr[qid] else 0\n",
    "        relCol.append(rel)\n",
    "    return relCol\n",
    "\n",
    "def readNDCG(file):\n",
    "    ndcg=[]\n",
    "    with open(file) as f:\n",
    "        for r in f:\n",
    "           ndcg.append(float(r.strip()))\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def fileLen(file):\n",
    "    with open(file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "def plotNDCG(x, y, title):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, y, color='r')\n",
    "    plt.xlabel('param')\n",
    "    plt.ylabel('mNDCG')\n",
    "    plt.grid(True)\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    plt.savefig(title + '.jpg')\n",
    "    # plt.show()\n",
    "\n",
    "def plotNDCG2(old,new,labels,x_labels,title):\n",
    "\n",
    "    ind = np.arange(len(old))\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots()\n",
    "    rec1=ax.bar(ind, old, width, color='r', label=labels[0],align='center')\n",
    "    rec2=ax.bar(ind+width, new, width, color='g', label=labels[1],align='center')\n",
    "\n",
    "    ax.set_xticks(ind)\n",
    "    plt.ylabel(\"mNDCG\")\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(title + '.jpg')\n",
    "\n",
    "def plotNDCG3(bars,x_labels,title):\n",
    "\n",
    "    ind = np.arange(len(bars))\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots()\n",
    "    rec1=ax.bar(ind, bars, width, color='r',align='center')\n",
    "\n",
    "    ax.set_xticks(ind)\n",
    "    plt.ylabel(\"mNDCG\")\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(title + '.jpg')\n",
    "\n",
    "class Preprocessor(object):\n",
    "    vocab = None\n",
    "\n",
    "    def __init__(self, vocab, stemmer=PorterStemmer()):\n",
    "        self.vocab = vocab\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def __stem_tokens(self, tokens):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(self.stemmer.stem(item))\n",
    "        return stemmed\n",
    "\n",
    "    # type: 0 - documents\n",
    "    # 1 - queries\n",
    "    def prep(self, text, type=0):\n",
    "        lowers = text.lower()\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", lowers)\n",
    "        tokens = nltk.word_tokenize(letters_only)\n",
    "        filtered = [w for w in tokens if not w in stopwords.words(\"english\")]\n",
    "        stemmed = self.__stem_tokens(filtered)\n",
    "        indexed = []\n",
    "        if stemmed:\n",
    "            for stem in stemmed:\n",
    "                # if case if we preprocess query WE DO NOT add new words to vocab.\n",
    "                if (type == 0):\n",
    "                    self.vocab.add(stem)\n",
    "                    indexed.append(self.vocab.wti[stem])\n",
    "                else:\n",
    "                    if (stem not in self.vocab.wti):\n",
    "                        indexed.append(self.vocab.wti['UNKNOWN'])\n",
    "                    else:\n",
    "                        indexed.append(self.vocab.wti[stem])\n",
    "        return indexed\n",
    "\n",
    "\n",
    "class IndexNode(object):\n",
    "    def __init__(self, docno, count):\n",
    "        self.docno = docno\n",
    "        self.count = count\n",
    "\n",
    "\n",
    "class InvertedIndex(object):\n",
    "    TDF = None  # columns: terms, rows:docs/query\n",
    "    DTF = None  # same but now columns are docs/queries, and rows are terms\n",
    "    C = 0  # total number of words in the collection\n",
    "\n",
    "    # m: the vocabulary length\n",
    "    # type: 0 - doc\n",
    "    # 1 - query\n",
    "    def __init__(self, collection, m, type=0):\n",
    "        # initializing array\n",
    "        self.TDF = [{} for j in range(m)]\n",
    "        self.DTF = {}\n",
    "        for id in collection:\n",
    "            document = collection[id]\n",
    "            text = document['fulltext'] if type == 0 else document\n",
    "            counts = collections.Counter(text)\n",
    "            for term in counts:\n",
    "                self.TDF[term][id] = counts[term]\n",
    "                if (id not in self.DTF):\n",
    "                    self.DTF[id] = np.zeros(m)\n",
    "                self.DTF[id][term] = counts[term]\n",
    "                # self.index[term].append(IndexNode(id, counts[term]))\n",
    "                self.C += counts[term]\n",
    "\n",
    "    # did: document id\n",
    "    def tf(self, terms, did):\n",
    "        n = len(terms)\n",
    "        results = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            fr = self.TDF[terms[i]][did] if did in self.TDF[terms[i]] else 0  # unseen words in a document get 0 frequency\n",
    "            results[i] = fr\n",
    "        return results\n",
    "\n",
    "\n",
    "    def df(self, terms):\n",
    "        results = {}\n",
    "        for word in terms:\n",
    "            results[word] = len(self.index[word])\n",
    "        return results\n",
    "\n",
    "    # collection term frequency\n",
    "    def ctf(self, terms):\n",
    "        n = len(terms)\n",
    "        results = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            results[i] = np.sum(self.TDF[terms[i]].values())\n",
    "        return results\n",
    "\n",
    "\n",
    "class QLMSmoothing(Enum):\n",
    "    none = 1\n",
    "    JR = 2  # Jelinek-Mercer\n",
    "    DP = 3  # Direchlet Prior\n",
    "    AD = 4  # Absolute discounting\n",
    "class PLMkernal(Enum):\n",
    "    gaussian=1\n",
    "    triangle=2\n",
    "    cosine=3\n",
    "    circle=4\n",
    "    passage=5\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    curr_ind = 0\n",
    "    itw = None  # index to word\n",
    "    wti = None  # word to index\n",
    "\n",
    "    def __init__(self):\n",
    "        self.curr_ind = 0\n",
    "        self.itw = []\n",
    "        self.wti = {}\n",
    "\n",
    "    def add(self, word):\n",
    "        if (word not in self.wti):\n",
    "            self.wti[word] = self.curr_ind\n",
    "            self.itw.append(word)\n",
    "            self.curr_ind += 1\n",
    "\n",
    "# general class for caching\n",
    "class Cache:\n",
    "    col=None\n",
    "    def __init__(self):\n",
    "        self.col=None\n",
    "    # r could be an empty array or hash object\n",
    "    def reset(self):\n",
    "        self.col=None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NDCG():\n",
    "    __cache = {}\n",
    "    __max_rel = None\n",
    "\n",
    "    def __init__(self, max_rel):\n",
    "        self.__max_rel = max_rel\n",
    "\n",
    "    # dcs: documents collections returned by the queries\n",
    "    # assuming that each document is represented by it's relevance!\n",
    "    # e.g. [1,4,3,2] <- collection of 4 documents\n",
    "    def run(self, dc):\n",
    "        k = len(dc)\n",
    "        Z = self.__getNorm(k)\n",
    "        res = 0\n",
    "        for r in range(k):\n",
    "            res += self.__score(dc[r], r + 1)\n",
    "        return Z * res\n",
    "\n",
    "    # computes ndcg on multiple collections of documents\n",
    "    # returns an array of values\n",
    "    def runOnCol(self, dcs):\n",
    "        n = len(dcs)\n",
    "        res = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            res[i] = self.run(dcs[i])\n",
    "        return res\n",
    "\n",
    "    def __getNorm(self, k):\n",
    "        if (k in self.__cache):\n",
    "            return self.__cache[k]\n",
    "        Z = 0\n",
    "        for r in range(1, k + 1):\n",
    "            Z += self.__score(self.__max_rel, r)\n",
    "        Z = 1 / Z\n",
    "        # storing in cache\n",
    "        self.__cache[k] = Z\n",
    "        return Z\n",
    "\n",
    "    # don't forget that rank starts from 1, and not 0!\n",
    "    def __score(self, rel, rank):\n",
    "        return (math.pow(2, rel) - 1) / math.log(1 + rank, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SignTest:\n",
    "    L = None  # number of loops\n",
    "\n",
    "    def __init__(self, loops):\n",
    "        self.L = loops\n",
    "\n",
    "    # A and B: are the queries with NDCGS\n",
    "    # returns true or false (two systems are significantly different or not)\n",
    "    # and p value\n",
    "    def run(self, A, B):\n",
    "        n = len(A)\n",
    "        actualMndcg = np.sum(A)/n - np.sum(B)/n\n",
    "\n",
    "        testMndcgs = []\n",
    "        if (n != len(B)):\n",
    "            raise ValueError('lengths of two systems has to be the same')\n",
    "        for l in range(self.L):\n",
    "            # swapping values from two arrays\n",
    "            for i in range(n):\n",
    "                pair=self.__select(A[i],B[i])\n",
    "                A[i] = pair[0]\n",
    "                B[i] = pair[1]\n",
    "            testMndcgs.append(np.sum(A)/n-np.sum(B)/n)\n",
    "        testMndcgs=np.array(testMndcgs)\n",
    "        p=(np.sum(testMndcgs>actualMndcg)+np.sum(testMndcgs<actualMndcg))/self.L\n",
    "        return (np.abs(actualMndcg)>np.abs(np.percentile(testMndcgs,95)),p)\n",
    "\n",
    "\n",
    "    # randomly selects a or b and returns either one\n",
    "    def __select(self, a, b):\n",
    "        return np.random.permutation([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test2():\n",
    "    sTest = SignTest(1000)\n",
    "    # test from slides\n",
    "    A=np.array([0.25,0.43,0.39,0.75,0.43,0.15,0.2,0.52,0.49,0.5])\n",
    "    B=np.array([0.35,0.84,0.15,0.75,0.68,0.85,0.8,0.5,0.58,0.75])\n",
    "    res = sTest.run(A, B)\n",
    "    print(res)\n",
    "\n",
    "    A=np.array([0.25,0.43,0.39,0.75,0.43,0.15,0.2,0.52,0.49,0.5])\n",
    "    B=np.array([0.25,0.43,0.39,0.75,0.43,0.15,0.2,0.52,0.49,0.5])\n",
    "    print(str(sTest.run(A, B)[0] == False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for implementation see the above support section\n",
    "\n",
    "def run3():\n",
    "\n",
    "    start=timer()\n",
    "    file = 'data/collection-small.txt'\n",
    "    docs, vocab = getDocuments(file)\n",
    "\n",
    "    print(\"- Took %.2f sec to load and preprocess docs \" % (timer() - start))\n",
    "    writeVocabulary(vocab,\"data/vocab.txt\")\n",
    "    writeDocuments(docs,\"data/prep-docs.txt\")\n",
    "\n",
    "\n",
    "    start = timer()\n",
    "    docs=getDocuments('data/prep-docs.txt',True)\n",
    "    vocab=readVocabulary('data/vocab.txt')\n",
    "    print(\"- Took %.2f sec to load preprocessed docs \" % (timer() - start))\n",
    "\n",
    "\n",
    "    print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for implementation see the above support section\n",
    "def test4():\n",
    "    file = 'data/collection-very-small-fake.txt'\n",
    "    docs, vocab = getDocuments(file)\n",
    "    ii = InvertedIndex(docs, len(vocab.itw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query language model\n",
    "class QLM():\n",
    "    smoothing = None\n",
    "    docs = None\n",
    "    II = None  # inverted index\n",
    "    params = None  # the set of parameters\n",
    "\n",
    "    def __init__(self, docs, II, params, smoothing=QLMSmoothing.none):\n",
    "        self.smoothing = smoothing\n",
    "        self.II = II\n",
    "        self.docs = docs\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "    # returns k top likely documents for a query\n",
    "    def findTop(self, qterms, k):\n",
    "        res = {}\n",
    "        for did in self.docs:\n",
    "            res[did] = self.__uniProb(qterms, did)\n",
    "        sorted_res = sorted(res.iteritems(), key=lambda (k, v): (-v, k))[:k]\n",
    "        final = []\n",
    "        for i in range(k):\n",
    "            final.append(sorted_res[i][0])\n",
    "        return final\n",
    "\n",
    "\n",
    "    # returns a unigram probab for the terms given some document model\n",
    "    # the method is polymorphic (in some sense)\n",
    "    # it returns different unigram probabilities for different smoothing methods\n",
    "    def __uniProb(self, terms, did):\n",
    "        d = len(self.docs[did]['fulltext'])  # number of terms in the document\n",
    "        if (self.smoothing == QLMSmoothing.none):\n",
    "            tfs = self.II.tf(terms, did)\n",
    "            res = (tfs / d)\n",
    "\n",
    "        if (self.smoothing == QLMSmoothing.JR):\n",
    "            C = self.II.C\n",
    "            ctfs = self.II.ctf(terms)\n",
    "            tfs = self.II.tf(terms, did)\n",
    "            res = self.params[0] * (tfs / d) + (1 - self.params[0]) * (ctfs / C)\n",
    "\n",
    "        if (self.smoothing == QLMSmoothing.DP):\n",
    "            C = self.II.C\n",
    "            ctfs = self.II.ctf(terms)\n",
    "            tfs = self.II.tf(terms, did)\n",
    "            probs = (ctfs / C)\n",
    "            res = (tfs + self.params[0] * probs) / (d + self.params[0])\n",
    "\n",
    "        if (self.smoothing == QLMSmoothing.AD):\n",
    "            tfs = self.II.tf(terms, did)\n",
    "            C = self.II.C\n",
    "            ctfs = self.II.ctf(terms)\n",
    "            du = len(np.unique(self.docs[did]['fulltext']))  # unique words in the document\n",
    "            probs = (ctfs / C)\n",
    "            res = ((np.maximum.reduce([tfs - self.params[0], np.zeros(len(terms))])) / d)\\\n",
    "                  + self.params[0] * du * probs / d\n",
    "        return np.sum(np.log(res))\n",
    "\n",
    "    # used only for crash tests\n",
    "    def testll(self, did):\n",
    "        fac = scipy.misc.factorial\n",
    "        terms = self.docs[did]['fulltext']\n",
    "        un = np.unique(terms)\n",
    "        L = len(terms)\n",
    "        tfs = self.II.tf(terms, did)\n",
    "        utfs = self.II.tf(un, did)\n",
    "        K = (fac(L) / np.prod(fac(utfs)))\n",
    "\n",
    "        return K * ((np.prod(tfs / L)))\n",
    "\n",
    "def test5():\n",
    "    file = 'data/collection-very-small.txt'\n",
    "    docs, vocab = getDocuments(file)\n",
    "    dii = InvertedIndex(docs, len(vocab.itw))\n",
    "    file = 'data/queries-small.txt'\n",
    "    queries = getQueries(file, vocab)\n",
    "\n",
    "    file = 'data/qrels-small.txt'\n",
    "    qdr = getDocQuerRel(file)\n",
    "\n",
    "    qlm = QLM(docs, dii, params=[0.5], smoothing=QLMSmoothing.AD)\n",
    "    res = {}\n",
    "    ndcg = NDCG(2)\n",
    "    for qid in queries:\n",
    "        dids = qlm.findTop(queries[qid], 10)\n",
    "        # map to relevance\n",
    "        relDocs = map_to_relevance(qid, dids, qdr)\n",
    "        res[qid] = {'docs': relDocs, 'ndcg': ndcg.run(relDocs)}\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Greed search over parameters space</h1>\n",
    "<h2>Jelinek-Mercer</h2>\n",
    "<img src=\"plots/jelinek_mercer.jpg\" style=\"width:400px\"></img>\n",
    "<h2>Direchlet prior</h2>\n",
    "<img src=\"plots/dirichlet_prior.jpg\" style=\"width:400px\"></img>\n",
    "<h2>Absolute discounting</h2>\n",
    "<img src=\"plots/absolute_discounting.jpg\" style=\"width:400px\"></img>\n",
    "\n",
    "\n",
    "<h1>Model comparison</h1>\n",
    "<table>\n",
    "<tr><th>Model</th><th>Optimal parameter</th><th>mNDCG</th></tr>\n",
    "<tr>\n",
    "<td>Jelinek-Mercer</td><td>0.2</td><td>0.164</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Direchlet prior</td><td>1000</td><td>0.281</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Absolute discounting</td><td>0.9</td><td>0.16</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 6</h1>\n",
    "In this part the author has been using sampling in several places in the optimization to speed-up computations. The method appears to produce overflows and underflows and therefore that has been taking into account during the implementation phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class TwoStageSmoothing():\n",
    "    params = None  # first one is miu(for Dirichet) and the second one is lambda (for JM)\n",
    "    pi=[]\n",
    "    docs = None  # the collection of documents\n",
    "    queries=None # the collection of queries\n",
    "    II = None  # inverted index\n",
    "    iter=None\n",
    "    perc=None\n",
    "    cache=None\n",
    "    debug=None\n",
    "\n",
    "    # iter: number of iterations in the training phases\n",
    "    # perc: percent of documents that are sampled each time e.g. 0.1\n",
    "    def __init__(self, docs,queries, II, params,perc, iter=100,debug=False):\n",
    "        self.II = II\n",
    "        self.docs = docs\n",
    "        self.queries=queries\n",
    "        self.params = params\n",
    "        self.perc=perc\n",
    "        self.iter=iter\n",
    "        self.pi=np.ones(len(docs))/len(docs)\n",
    "        self.debug=debug\n",
    "        self.cache=Cache() # we shall cache denom. of E-step\n",
    "\n",
    "\n",
    "    def findTop(self, qterms, k):\n",
    "        res = {}\n",
    "        for did in self.docs:\n",
    "            res[did] = self.__uniProb(qterms, did)\n",
    "        sorted_res = sorted(res.iteritems(), key=lambda (k, v): (-v, k))[:k]\n",
    "        final = []\n",
    "        for i in range(k):\n",
    "            final.append(sorted_res[i][0])\n",
    "        return final\n",
    "\n",
    "    def __uniProb(self, terms, did):\n",
    "        # estimating collection based normalization of non-discr. terms\n",
    "        # a.k.a Jelinek-Mercel\n",
    "        cProbs = self.jm(terms)\n",
    "\n",
    "        # estimating document prob. (Dirichlet)\n",
    "        dProbs=self.dir(terms,did)\n",
    "\n",
    "        res=(1-self.params[1])*dProbs +self.params[1]*cProbs\n",
    "\n",
    "        return np.sum(np.log(res))\n",
    "\n",
    "    # log likelihood for documents model\n",
    "    def dLL(self):\n",
    "        res=0\n",
    "        for did in self.docs:\n",
    "            terms=self.docs[did]['fulltext']\n",
    "            d=len(terms)\n",
    "            tfs = self.II.tf(terms, did)\n",
    "            cProbs = self.jm(terms)\n",
    "            dProbs= (tfs -1 + self.params[0] * cProbs) / (d -1 + self.params[0])\n",
    "            res+=np.sum(np.log(dProbs))\n",
    "        return res\n",
    "\n",
    "    # log likelihood for queries model\n",
    "    def qLL(self):\n",
    "        res=1\n",
    "        for qid in self.queries:\n",
    "            temp_res=0\n",
    "            for i,did in enumerate(self.docs):\n",
    "                pi=self.pi[i]\n",
    "                terms=self.queries[qid]\n",
    "                dProd=self.dir(terms,did) # 1st role prob, prob of query qid\n",
    "                cProb = self.jm(terms) # the JM 2nd role prob\n",
    "                temp_res+=pi*np.prod((1-self.params[1])*dProd + self.params[1]*cProb)\n",
    "            res*=temp_res\n",
    "        return res\n",
    "\n",
    "\n",
    "    # p(ids| d_i) #\n",
    "    # returns a list of dirichlet smoothed probabilities for term ids\n",
    "    def dir(self,terms,did):\n",
    "        d=len(self.docs[did]['fulltext'])\n",
    "        tfs = self.II.tf(terms, did)\n",
    "        cProbs = self.jm(terms)\n",
    "        res=(tfs+ self.params[0] * cProbs) / (d + self.params[0])\n",
    "        if(np.array(res<0).any() or np.array(res>1).any()):\n",
    "            print('error in dir')\n",
    "        return res\n",
    "\n",
    "    # return JM list of probabilities\n",
    "    # note that JM play the second role, similar to TF-IDF\n",
    "    def jm(self,terms):\n",
    "        C = self.II.C\n",
    "        return self.II.ctf(terms)/C\n",
    "\n",
    "    def train(self,llEveryIter):\n",
    "        print('phase 1 of training')\n",
    "        self.__trainStage1(llEveryIter)\n",
    "        print('phase 2 of training')\n",
    "        self.__trainStage2(llEveryIter)\n",
    "\n",
    "\n",
    "    def __trainStage1(self,n):\n",
    "        col=[]\n",
    "        par=[]\n",
    "        for i in range(1,self.iter+1):\n",
    "            deriv=self.__dGrads()\n",
    "            self.params[0]-=deriv[0]/deriv[1]\n",
    "            if(self.debug==True and i%n==0):\n",
    "                LL=self.dLL()\n",
    "                col.append(LL)\n",
    "                par.append(self.params[0])\n",
    "                print('iter # '+str(i))\n",
    "                print('dLL is : '+str(LL))\n",
    "        self.__plotProgress(col,par,'Phase 1 (miu1) dLL')\n",
    "\n",
    "    def __trainStage2(self,n):\n",
    "        col=[]\n",
    "        par=[]\n",
    "        for j in range(1,self.iter+1):\n",
    "            # E-step\n",
    "            for qid in random.sample(self.queries,20):\n",
    "                new_pi=[]\n",
    "                self.cache.reset() # resetting cache\n",
    "                for i in range(len(self.pi)):\n",
    "                    new_pi.append(self.__E(i,qid))\n",
    "                self.pi=new_pi\n",
    "            for qid in random.sample(self.queries,5):\n",
    "            # M-step\n",
    "                self.params[1]=self.__M(qid)\n",
    "            if(j%n==0):\n",
    "                LL=self.qLL()\n",
    "                col.append(LL)\n",
    "                par.append(self.params[1])\n",
    "                print('iter # '+str(j))\n",
    "                print('qLL is : '+str(LL))\n",
    "        self.__plotProgress(col,par,'Phase 2 (lambda1) qLL')\n",
    "\n",
    "\n",
    "\n",
    "    # returns a tuple of 1st order derivative and 2nd order derivative\n",
    "    def __dGrads(self):\n",
    "        grad1=0\n",
    "        grad2=0\n",
    "        samp = random.sample(self.docs, int(math.floor(self.perc*len(self.docs))))\n",
    "\n",
    "        for did in samp:\n",
    "            terms=self.docs[did]['fulltext']\n",
    "            d=len(terms)\n",
    "            tfs = self.II.tf(terms, did)\n",
    "            cProbs = self.jm(terms)\n",
    "\n",
    "            # temp collector for 1st order der\n",
    "            tRes1= (tfs*((d-1)*cProbs-tfs+1))/\\\n",
    "                 ((d-1+self.params[0])*(tfs-1 + self.params[0]*cProbs))\n",
    "\n",
    "            # temp collector of 2nd order der\n",
    "            tRes2=(tfs*np.power(((d-1)*cProbs-tfs+1),2))/\\\n",
    "                 math.pow((d-1+self.params[0]),2)*np.power(tfs-1+self.params[0]*cProbs,2)\n",
    "\n",
    "            grad1+=np.sum(tRes1)\n",
    "            grad2-=np.sum(tRes2)\n",
    "\n",
    "        return (grad1,grad2)\n",
    "\n",
    "    def __E(self,i,qid):\n",
    "\n",
    "        # computing numerator\n",
    "        did=self.docs.keys()[i]\n",
    "        terms=self.queries[qid]\n",
    "        try:\n",
    "            num=self.pi[i]*np.prod((1-self.params[1])*self.dir(terms,did)+self.params[1]*self.jm(terms))\n",
    "        except ValueError:\n",
    "            print('value error in E')\n",
    "        # computing denominator\n",
    "        if(self.cache.col==None):\n",
    "            denom=0\n",
    "                # over all docs\n",
    "            for k,did in enumerate(self.docs):\n",
    "                denom+=self.pi[k]*np.prod((1-self.params[1])*self.dir(terms,did)+self.params[1]*self.jm(terms))\n",
    "            self.cache.col=denom\n",
    "        else:\n",
    "             denom=self.cache.col\n",
    "\n",
    "        if(denom==0 or num==0):\n",
    "            res=0.0001 # if we set it to 0 there is no way back.\n",
    "        else:\n",
    "            res=num/denom\n",
    "\n",
    "        # this twoStage model SUFFERS FROM OVERFLOWS!\n",
    "        if(math.isnan(res) or math.isinf(res)):\n",
    "            res=0.0001  # if we set it to 0 there is no way back.\n",
    "           # print('error in E-step')\n",
    "        return res\n",
    "\n",
    "\n",
    "    def __M(self,qid):\n",
    "        Q=len(self.queries)\n",
    "        res=0\n",
    "        for i,did in enumerate(self.docs):\n",
    "            terms=self.queries[qid]\n",
    "            num = self.params[1]*self.jm(terms)\n",
    "            denom= (1-self.params[1])*self.dir(terms,did)+self.params[1]*self.jm(terms)\n",
    "            # avoid zeroes\n",
    "            ind=np.where(denom==0)\n",
    "            if len(ind[0])!=0:\n",
    "                for i in ind[0]:\n",
    "                    num[i]=0\n",
    "                    denom[i]=1\n",
    "            res+=self.pi[i]*np.sum(num/denom)\n",
    "            if(res/Q==0 or math.isnan(res/Q)):\n",
    "                print('error in M step')\n",
    "        return res/Q\n",
    "\n",
    "    def __plotProgress(self,col,iter,title):\n",
    "        fig = plt.figure()\n",
    "        plt.plot(iter,col, color='r')\n",
    "        plt.xlabel('param')\n",
    "        plt.ylabel('mNDCG')\n",
    "        plt.grid(True)\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        plt.savefig(title.replace(' ','_').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run6():\n",
    "    docs=getDocuments('data/prep-docs.txt',True,20000)\n",
    "    vocab=readVocabulary('data/vocab.txt')\n",
    "    dii = InvertedIndex(docs, len(vocab.itw))\n",
    "    file = 'data/queries-small.txt'\n",
    "    queries = getQueries(file, vocab)\n",
    "\n",
    "    file = 'data/qrels-small.txt'\n",
    "    qdr = getDocQuerRel(file)\n",
    "\n",
    "    twoStage=TwoStageSmoothing(docs,queries,dii,[1,0.5],perc=0.1,iter=2,debug=False)\n",
    "    # train two parameters\n",
    "    twoStage.train(5)\n",
    "    ndcg=NDCG(2)\n",
    "    ndcgRes = []\n",
    "    Q=len(queries)\n",
    "    for qid in queries:\n",
    "        dids = twoStage.findTop(queries[qid], 10)\n",
    "        # map to relevance\n",
    "        relDocs = map_to_relevance(qid, dids, qdr)\n",
    "        ndcgRes.append(ndcg.run(relDocs))\n",
    "    writeNDCG(ndcgRes,\"ndcg/twoStage.txt\")\n",
    "    print(str(np.sum(twoStage.pi)))\n",
    "    print('with parameters: '+str(twoStage.params)+' mndcg is: '+str(np.sum(ndcgRes)/Q))\n",
    "\n",
    "    # optimized params: \\mu =1.0007869864702941, \\lambda=1.6292281580652353e-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>\n",
    "\n",
    "The model is much slower than simple QLM with smoothing, therefore it was not realistic to run the model on full dataset, and only 20,000 documents have been used with sampling during the training phase. It has been noticed that the model is sensitive to the number of documents it is trained on and therefore the produced result most likely is sub-optimal. In addition, multiple iterations in the training phase(epoches) has been notice to produce positive effect to the final evaluation measure (mNDCG).\n",
    "\n",
    "\n",
    "<h2>Optimized Parameters</h2>\n",
    "\\mu = 1.0007869864702941<br>\n",
    "\\lambda =\\lambda=1.6292281580652353e-10\n",
    "\n",
    "<h2>mNDCG</h2>\n",
    "mNDCG=0.209192471774<br>\n",
    "But as has been noticed, it is possible to improve the result by running it on more data. And as models from step 3 have been trained on more data, we can't objectively compare the current and previous models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 7 (Bonus)<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Position Language Model\n",
    "class PLM():\n",
    "    docs=None\n",
    "    queries=None\n",
    "    kernal=None\n",
    "    dii=None # document inverted index\n",
    "    sigma=None\n",
    "\n",
    "    def __init__(self,docs,queries,dii,params,kernal=PLMkernal.gaussian):\n",
    "        self.docs=docs\n",
    "        self.queries=queries\n",
    "        self.kernal=kernal\n",
    "        self.dii=dii\n",
    "        self.sigma=params\n",
    "\n",
    "    def __score(self,qid,did,i):\n",
    "        n=len(qid)\n",
    "        plms=self.__plmProb(self.queries[qid],did,i)\n",
    "        return -np.sum((1/n)*np.log((1/n)/plms))\n",
    "\n",
    "    # the special tf\n",
    "    # i: position\n",
    "    # returns a list of tfs for the terms\n",
    "    def __tf(self,terms,i):\n",
    "        res=np.zeros(len(terms))\n",
    "        for j,did in enumerate(self.docs):\n",
    "            tfs=self.dii.tf(terms,did)\n",
    "            k=self.__k(i,j)\n",
    "            res+=tfs*k\n",
    "        return res\n",
    "\n",
    "    # the actual position language model\n",
    "    # terms: a list of terms\n",
    "    # i: position\n",
    "    def __plmProb(self,terms,did,i):\n",
    "        tfs = self.__tf(terms,i)\n",
    "        # computing denominator\n",
    "        # in optimized way\n",
    "        Z = 0\n",
    "        for j in range(len(self.docs)):\n",
    "            Z+=self.__k(i,j)\n",
    "        return tfs/Z\n",
    "\n",
    "    def reEval(self,qid,dids):\n",
    "            score={}\n",
    "            for did in dids:\n",
    "                score[did]=np.max([self.__score(qid,did,i) for i in range(len(dids))])\n",
    "            res=sorted(score.items(), key=operator.itemgetter(1),reverse=True)\n",
    "            res= [r[0] for r in res]\n",
    "            return res\n",
    "\n",
    "    # polynomic kernal function\n",
    "    def __k(self,i,j):\n",
    "\n",
    "        # 1. gaussian kernal\n",
    "        if(self.kernal==PLMkernal.gaussian):\n",
    "            return math.exp(-math.pow(i-j,2)/2*math.pow(self.sigma,2))\n",
    "\n",
    "        # 2. triangle kernal\n",
    "        if(self.kernal==PLMkernal.triangle):\n",
    "            if(np.abs(i-j)<=self.sigma):\n",
    "                return 1- np.abs(i-j)/self.sigma\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # 3. cosine (hamming ) kernal\n",
    "        if(self.kernal==PLMkernal.cosine):\n",
    "            if(np.abs(i-j)<=self.sigma):\n",
    "                return (1+math.cos((np.abs(i-j)*math.pi)/self.sigma))/2\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # 4. circle kernal\n",
    "        if(self.kernal==PLMkernal.circle):\n",
    "             if(np.abs(i-j)<=self.sigma):\n",
    "                 return math.sqrt(1-math.pow((np.abs(i-j)/self.sigma),2))\n",
    "             else:\n",
    "                return 0\n",
    "\n",
    "        # 5. passage\n",
    "        if(self.kernal==PLMkernal.passage):\n",
    "            if(np.abs(i-j)<=self.sigma):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>\n",
    "<img src=\"plots/PLM_and_direchlet_LM_eval_(mNDCG).jpg\" style=\"width:400px\"></img>\n",
    "<h1>Conclusions</h1>\n",
    "As has been noticed during the experiments the kernals do not different from each other in the rescored documents mNDCG. The reason for this might be that we have only let the model to rescore the obtained documents via Dirichlet smoothed QLM, but it could have different effect if it would retrieve top 10 matches itself.\n",
    "\n",
    "Computational-wise the model is more expensive than simple Dirichlet smoothed QLM and as we can see from the results, the latter is more accurate. Once again, the results might be skewed as we did only re-scoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 8</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    docs=None\n",
    "    vocab=None\n",
    "    def __init__(self, docs,vocab):\n",
    "         self.docs=docs\n",
    "         self.vocab=vocab\n",
    "\n",
    "    def __iter__(self):\n",
    "            for did in self.docs:\n",
    "                yield [self.vocab.itw[int(x)] for x in self.docs[did]['fulltext']]\n",
    "\n",
    "def run8():\n",
    "    docs=getDocuments('data/prep-docs.txt',True)\n",
    "    vocab=readVocabulary('data/vocab.txt')\n",
    "    sentences = MySentences(docs,vocab) # a memory-friendly iterator\n",
    "    model = gensim.models.Word2Vec(sentences,min_count=1,workers=5,window=10,size=200,negative=15,sg=1)\n",
    "    print('evaluation')\n",
    "    model.accuracy('eval/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>\n",
    "\n",
    "The model has been tried with different parameters, and tested on questions test-set. It appeared that skip-gram outperforms CBOW and 200 is a good choise for the number of hidden units. In addition, negative sampling has been used to speed-up computations. In constrast to hierarchical softmax, negative sampling makes it possible to use output represention of words, while the former makes the output matrix correspond to inner node weights on the Huffman's tree. The number of negative samples has been noticed affect positivly the evaluation measure, and the number has been set to 15 to train more accurate model. The context window size has been set to 10, similar to the original Mikolov's paper and the minimum number of words set to 1, while the default was 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 9</h1>\n",
    "As it was permitted in the homework assignment to use doc2vec(paragraph2vec), the author has used ginsem implementation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluates the document ranks for queries\n",
    "class VecEvaluator():\n",
    "    docs=None # documents with vectors\n",
    "    queries = None # queries with vectors\n",
    "\n",
    "    def __init__(self,docs,queries):\n",
    "        self.docs=docs\n",
    "        self.queries=queries\n",
    "\n",
    "    # re-evaluates based on cosine similarity\n",
    "    def eval(self,qid,dids):\n",
    "        cos={}\n",
    "        for did in dids:\n",
    "            cos[did]=1-spatial.distance.cosine(self.docs[did]['vec'], self.queries[qid]['vec'])\n",
    "        res=sorted(cos.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        #res=sorted(cos,key=cos.get, reverse=True)\n",
    "        res= [r[0] for r in res]\n",
    "        return res\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    col=None # collection of documents or queries\n",
    "    vocab=None\n",
    "    type=None\n",
    "    # type: 0 for documents, 1 for queries\n",
    "    def __init__(self, col,vocab,type=0):\n",
    "         self.col=col\n",
    "         self.vocab=vocab\n",
    "         self.type=type\n",
    "    def __iter__(self):\n",
    "        for i,id in enumerate(self.col):\n",
    "            if(self.type==0):\n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=[self.vocab.itw[int(x)] for x in self.col[id]['fulltext']],tags=['SENT_%s' % i])\n",
    "            if(self.type==1):\n",
    "                words=[self.vocab.itw[int(x)] for x in self.col[id]]\n",
    "                yield gensim.models.doc2vec.LabeledSentence(words=words,tags=['SENT_%s' % i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run9():\n",
    "    docs=getDocuments('data/prep-docs.txt',True)\n",
    "    vocab=readVocabulary('data/vocab.txt')\n",
    "    queries = getQueries('data/queries-small.txt', vocab)\n",
    "    dSent = LabeledLineSentence(docs,vocab) # a memory-friendly iterator\n",
    "    qSent = LabeledLineSentence(queries,vocab,1) # a memory-friendly iterator\n",
    "\n",
    "    print('starting training')\n",
    "    dModel = gensim.models.Doc2Vec(dSent,workers=7,window=10,size=100,alpha=0.1,negative=15,iter=1)\n",
    "    qModel = gensim.models.Doc2Vec(qSent,workers=5,window=10,size=100,alpha=0.1,negative=15,iter=1)\n",
    "\n",
    "    print ('finished training')\n",
    "    # storing vector representations\n",
    "    for i,did in enumerate(docs):\n",
    "        docs[did]['vec']=dModel.docvecs['SENT_%s' % i]\n",
    "    for i,qid in enumerate(queries):\n",
    "        terms=queries[qid]\n",
    "        queries[qid]={'fulltext':terms,'vec':qModel.docvecs['SENT_%s' % i]}\n",
    "\n",
    "    file = 'data/qrels-small.txt'\n",
    "    qdr = getDocQuerRel(file)\n",
    "    ev= VecEvaluator(docs,queries) # evaluator\n",
    "\n",
    "    ndcgRes=[]\n",
    "    ndcg = NDCG(2)\n",
    "    Q=len(queries)\n",
    "    # taking top 10 documents for each query based on similarity\n",
    "    for qid in queries:\n",
    "        dids=  ev.eval(qid,docs.keys())\n",
    "\n",
    "         # map to relevance\n",
    "        relDocs1 = map_to_relevance(qid, dids, qdr)\n",
    "        ndcgRes.append(ndcg.run(relDocs1))\n",
    "\n",
    "\n",
    "    writeNDCG(ndcgRes,\"ndcg/vectorized.txt\")\n",
    "    print(\"mNDCG is: \")\n",
    "    print(str(np.sum(ndcgRes) / Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>\n",
    "The model shares similarities with the previous one, and therefore some settings such as\n",
    "negative sampling with 15 n.s. have been used. \n",
    "The number of hidden units has been set to 100 as a trade-off between quality of vectors and the time-complexity. The distributed bag of words from the paper produced better result and the same has been supported by the author's own results, it has been used in later phases. In addition, the author tried different learning rate values, and 0.1 appears to be a good choise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 10 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run10():\n",
    "    docs=getDocuments('data/prep-docs.txt',True)\n",
    "    vocab=readVocabulary('data/vocab.txt')\n",
    "    dii = InvertedIndex(docs, len(vocab.itw))\n",
    "\n",
    "    queries = getQueries('data/queries-small.txt', vocab)\n",
    "    dSent = LabeledLineSentence(docs,vocab) # a memory-friendly iterator\n",
    "    qSent = LabeledLineSentence(queries,vocab,1) # a memory-friendly iterator\n",
    "\n",
    "    print('training dModel')\n",
    "    dModel = gensim.models.Doc2Vec(dSent,workers=6,min_count=1,window=10,size=200,alpha=0.1,negative=15,iter=3)\n",
    "\n",
    "    print('training qModel')\n",
    "    qModel = gensim.models.Doc2Vec(qSent,workers=6,min_count=1,window=10,size=200,alpha=0.1,negative=15,iter=3)\n",
    "\n",
    "    print ('finished training')\n",
    "    # storing vector representations\n",
    "    for i,did in enumerate(docs):\n",
    "        docs[did]['vec']=dModel.docvecs['SENT_%s' % i]\n",
    "    for i,qid in enumerate(queries):\n",
    "        terms=queries[qid]\n",
    "        queries[qid]={'fulltext':terms,'vec':qModel.docvecs['SENT_%s' % i]}\n",
    "\n",
    "    print('finished extracting vectors')\n",
    "\n",
    "    file = 'data/qrels-small.txt'\n",
    "    qdr = getDocQuerRel(file)\n",
    "\n",
    "    ## optimals are: JR - 0.2, Dir: 1000, AD: 0.9\n",
    "\n",
    "    sms = [QLMSmoothing.JR, QLMSmoothing.DP, QLMSmoothing.AD]\n",
    "    smsTitles = ['Jelinek-Mercer', 'Dirichlet_prior', 'Absolute_discounting']\n",
    "    params = [0.2, 1000, 0.9]\n",
    "    ndcg = NDCG(2)\n",
    "    re= VecEvaluator(docs,queries) # re-evaluator\n",
    "    oldCol = np.zeros(len(smsTitles))\n",
    "    newCol = np.zeros(len(smsTitles))\n",
    "    Q = len(queries)\n",
    "\n",
    "    for i in range(len(sms)):\n",
    "        sm = sms[i]\n",
    "        param=params[i]\n",
    "        qlm = QLM(docs, dii, params=[param], smoothing=sm)\n",
    "        ndcg1Res = []\n",
    "        ndcg2Res = []\n",
    "        for qid in queries:\n",
    "            dids1 = qlm.findTop(queries[qid]['fulltext'], 10) # original dids\n",
    "            dids2=  re.eval(qid,dids1)\n",
    "\n",
    "            # map to relevance\n",
    "            relDocs1 = map_to_relevance(qid, dids1, qdr)\n",
    "            relDocs2 = map_to_relevance(qid, dids2, qdr)\n",
    "\n",
    "            ndcg1Res.append(ndcg.run(relDocs1))\n",
    "            ndcg2Res.append(ndcg.run(relDocs2))\n",
    "\n",
    "        writeNDCG(ndcg1Res,\"ndcg/\"+smsTitles[i].lower()+\".txt\")\n",
    "        writeNDCG(ndcg2Res,\"ndcg/\"+smsTitles[i].lower()+\"_vectorized.txt\")\n",
    "        oldCol[i]=np.sum(ndcg1Res) / Q\n",
    "        newCol[i]=np.sum(ndcg2Res) / Q\n",
    "\n",
    "    print(newCol)\n",
    "    plotNDCG2(oldCol,newCol,x_labels=smsTitles,labels=[\"QLM\",\"Vector-based re-eval\"],title=\"QLM and Vector-based eval (mNDCG)\")\n",
    "\n",
    "run10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>\n",
    "<img src=\"plots/QLM_and_vector_based_eval(mNDCG).jpg\" style=\"width:400px\"></img>\n",
    "\n",
    "<h1>Conclusions</h1>\n",
    "As has been observed, vector-based re-evaluation produces worse mNDCG. But as previously, the model has been used only for re-ranking, and therfore if used on the full set of documents instead of top 10 retrieved by smootheded QLMs it could have produced better results.\n",
    "Nevertheless, we can see that it has very close to the original mNDCG and it has been observed that more iterations produce better results, and therefore it has been set to 3 after several tests as a trade-off between quality of vectors and computation time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 11</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the base-line for comparisons the smoothed Dirichlet QLM has been chosen. \n",
    "<table>\n",
    "<tr>\n",
    "<th>Model</th><th>mNDCG</th><th>p-value</th><th>sign. different?</th></tr>\n",
    "<tr>\n",
    "<td>Dirichlet</td><td><b>0.284</b></td><td>-</td><td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Absolute discounting</td><td>0.164</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Jelinek-Mercer</td><td>0.116</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Dirichlet vectorized (re-rank)</td><td>0.264</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Absolute discounting vectorized (re-rank)</td><td>0.134</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Jelinek-Mercer vectorized (re-rank)</td><td>0.102</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TwoStage smoothing</td><td>0.21</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PLM circle</td><td>0.25</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PLM cosine</td><td>0.25</td><td>1</td><td>True</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PLM gaussian</td><td>0.25</td><td>0.99</td><td>False</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PLM passage</td><td>0.25</td><td>0.97</td><td>False</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PLM triangle</td><td>0.25</td><td>0.75</td><td>False</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dirichlet smoothed QLM showed the best mNDCG results but required to perform\n",
    "greed search over a space, which could be inefficient for large documents and quaries collection.\n",
    "An alternative could be to learn the \\mu parameter using machine learning techniques\n",
    "2. Vectorized re-ranking of 3 smoothed methods showed worse results than the originals. Unfortunately, when the vector-based method has been used to do a pure top 10 documents ranking for each query, it has produced close to 1% mNDCG results. The problem could reside in the fact that word2vec techniques are less efficient for IR tasks, the same for entailment problems. It is possible that the word2vec model has to be trained with more IR problem oriented objective function. \n",
    "3. Position Language models showed also worse re-ranking results, but hypothesis testing indicated that 3/4 models are not significantly different. Unfortunately, it has not been possible to perform a pure top 10 documents ranking for each query to see how well the models perform.\n",
    "4. TwoStage smoothing is a more complex method than the 3rd step smoothing methods, but it encodes both Dirichlet prior and Jelinek-Mercel smoothing,as the former is better for unknown words, and the latter for balancing frequent words in the collection. Unfortunately, the model requires two stage training which is time consuming and therefore it was not possible to use the full collection of documents to perform ranking. Nevertheless, it has been noticed that model improves performences significantly when 10,000 documents are changed to 20,000. Thus, it could a good candidate for futher research. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
